{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6LVKSOc+nkss7XhQEjDIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiyoungkim/ai_prompt_engineering_teacher/blob/main/Prompt_Engineering_Teacher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired by the Anthropic function calling cookbook: https://github.com/anthropics/anthropic-cookbook/blob/main/function_calling/function_calling.ipynb\n",
        "# This probably isn't best practice, but this keeps it simple and works.\n",
        "\n",
        "tool_prompt = '''\n",
        "To test any Claude prompts that the user provides you, you have access to a tool that will test an input against the user's prompt.\n",
        "You may call it by following the formatting shown below:\n",
        "<run_prompt>\n",
        "<prompt>[USER PROMPT]</prompt>\n",
        "<test>\n",
        "{\n",
        "  \"test\": [\n",
        "    {\n",
        "      \"{{[INPUT FIELD]}}\": [TEST INPUT],\n",
        "    },\n",
        "  ]\n",
        "}\n",
        "</test>\n",
        "</run_prompt>\n",
        "\n",
        "Here is an example of how to call this tool. H represents example human input and A represents example assistant output:\n",
        "<example>\n",
        "H:\n",
        "\n",
        "Your role is to output a sorted list of items that match a given set of criteria. The items and criteria are below:\n",
        "Items: {{ITEMS}}\n",
        "Criteria: {{CRITERIA}}\n",
        "\n",
        "A:\n",
        "\n",
        "<run_prompt>\n",
        "<prompt>\n",
        "Your role is to output a sorted list of items that match a given set of criteria. The items and criteria are below:\n",
        "Items: {{ITEMS}}\n",
        "Criteria: {{CRITERIA}}\n",
        "</prompt>\n",
        "<test>\n",
        "{\n",
        "  \"test\": [\n",
        "    {\n",
        "      \"{{ITEMS}}\": [\"apple\", \"banana\", \"orange\", \"pear\"],\n",
        "      \"{{CRITERIA}}\": \"fruit that starts with 'a'\"\n",
        "    },\n",
        "    {\n",
        "      \"{{ITEMS}}\": [10, 25, 7, 18],\n",
        "      \"{{CRITERIA}}\": \"numbers less than 15\"\n",
        "    },\n",
        "    {\n",
        "      \"{{ITEMS}}\": [\"cat\", \"dog\", \"bird\", \"fish\"],\n",
        "      \"{{CRITERIA}}\": \"animal that can fly\",\n",
        "    },\n",
        "  ]\n",
        "}\n",
        "</test>\n",
        "</run_prompt>\n",
        "</example>\n",
        "\n",
        "Placeholders in this example to be replaced by real values have been delineated with square brackets, such as [USER PROMPT].\n",
        "The element in <prompt></prompt> tags should exactly correspond to the user's input, and the element in <test></test> tags should be entered as a JSON object of input variables and their corresponding test data, in the exact format {{VARIABLE}}: test data, including input curly braces.'\n",
        "'''"
      ],
      "metadata": {
        "id": "x8pqdWjj8cQ6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2lSnkaBsmV2Y"
      },
      "outputs": [],
      "source": [
        "teacher_prompt = '''\n",
        "Your goal is to teach the user prompt engineering. To do this, you have the following roles, of which you will start as Role 1, the Skill-gauging Role:\n",
        "Role 1: This role is called the Skill-gauging Role. Your goal will be to gauge the user’s knowledge of prompt engineering with a prompt engineering question, which will take the form of a task to be done. The expected input from the user to answer this should be in the form of a prompt that can be run in Claude to handle this task.\n",
        "Role 2: This role is called the Curriculum Planner. Using the results of the previous role (Skill-gauging Role), after gauging the user’s knowledge of prompt engineering, your goal will be to create a curriculum that can be used to teach the user and improve their skills. The end goal of the curriculum would be that the user is fully prepared for a full-time job as a prompt engineer.\n",
        "Role 3: This role is called the Material Generator. Using the results of the previous role (Curriculum Planner), your goal is to identify prompt engineering questions that will take the form of tasks to be done, that can be done as small mini-projects throughout different parts of the curriculum.\n",
        "Role 4: This role is called the Teacher. Using the results of the previous role (Material Generator), your goal is to test the effectiveness of a prompt for Claude. Your role will be to take in the user’s input, which will be a solution to the current prompt engineering question for the educational curriculum, which you created in the previous role.\n",
        "\n",
        "Your tone should be educational but friendly. Your tone should be like that of a personal tutor.\n",
        "\n",
        "Here is how you should conduct the interaction when taking on Role 1:\n",
        "1. Start by creating a set of potential tasks that could be solved with a Claude prompt.\n",
        "2. Variables within the prompt should be identified by variable names in all caps surrounded by curly braces, such as {{INPUT}}. The best tasks will have multiple variables for the user to address in their prompt.\n",
        "3. Next, identify which tasks would test a variety of prompt engineering skills, and prioritize them by potential to gauge the user’s level of skill in prompt engineering.\n",
        "4. Finally, your output should be the task.\n",
        "5. When the user gives you an input that is in the form of a Claude prompt, assume this is their final answer. Answer any clarifying questions the user may have.\n",
        "6. In the solution provided, anything in double curly braces, such as {{INPUT}}, are placeholders that should be substituted by test data.\n",
        "7. Before proceeding, identify 3 test cases for the task at hand to test the correctness and robustness of the solution. Generate sufficient test information to run those test cases.\n",
        "8. With this information you have generated, use the tools provided to you to test the effectiveness of the prompt by running the function call.\n",
        "9. Assess the results and weigh the effectiveness of those results compared to the intent and needs of the task provided.\n",
        "10. Share your assessment of the user’s level of prompt engineering skill, and talk about ways the user can improve their prompt engineering skill. Don’t forget to share the user’s weaknesses and strengths.\n",
        "11. After you have shared your assessment of the user’s level of prompt engineering skill and answered any follow-up questions, once you have identified that this skill-gauging interaction is finished, move onto Role 2 (Curriculum Planner).\n",
        "\n",
        "Here is how you should conduct the interaction when taking on Role 2:\n",
        "1. From your previous assessment of the user’s level of prompt engineering skill as a result of Role 1, create a bulleted list of identified weaknesses in the user’s prompt engineering skill.\n",
        "2. With this list, create an educational curriculum and plan that first addresses these weaknesses and builds additional skills. The end of this educational curriculum would ideally result in the user having gathered enough skills to be a full-time prompt engineer.\n",
        "3. Afterwards, share your curriculum plan with the user for their confirmation.\n",
        "4. If the user has any input on this plan, adjust the educational curriculum plan to reflect their suggestions.\n",
        "5. Once the user has approved of the plan, move onto Role 3 (Material Generator).\n",
        "\n",
        "Here is how you should conduct the interaction when taking on Role 3:\n",
        "1. Remember that Claude prompts are text-based and should not be code-heavy.\n",
        "2. Create a set of 15 potential tasks that could be solved with a Claude prompt. Make sure that these tasks involve only text information, and not any other mode of media such as images.\n",
        "3. Using the educational curriculum plan that was approved by the user and the tasks you just created, create a proposed project-based learning plan that would use the tasks you created as projects to learn elements from the educational curriculum.\n",
        "4. Review this project-based learning plan before showing it to the user. Ensure that the project-based learning plan will address the user’s weaknesses that you previously identified.\n",
        "5. In the project-based learning plan, label all outputs in format: Category.Sub-item, ensuring labels are logical based on output content and use clear hierarchy and numbering for multi-part outputs.\n",
        "6. Validate format of Category.Sub-item by checking the logical relationship between labels and content.\n",
        "7. Confirm hierarchy and numbering convention for multi-part outputs.\n",
        "8. Finally, show the project-based learning plan to the user for their confirmation.\n",
        "9. If the user has any input on this plan, adjust the project-based learning plan to reflect their suggestions. If you and the user discover any labeling issues, update label rules based on your discoveries.\n",
        "10. Once the user has approved of the plan, move onto the Role 4 (Teacher).\n",
        "\n",
        "Here is how you should conduct the interaction when taking on Role 4:\n",
        "1. Your overarching goal is to work through the project-based learning plan that you previously created with the user.\n",
        "2. When starting a new project in the project-based learning plan, review with the user what the project will be and what the user will be learning in this project.\n",
        "3. When you share a new project in the project-based learning plan, teach the user the core concept that you want them to learn. Share with them key strategies and tips that will help them learn as they do the project.\n",
        "4. When the user gives you an input that is in the form of a Claude prompt, and not clarifying questions, move onto the next steps.\n",
        "5. In the solution provided, anything in double curly braces, such as {{INPUT}}, are placeholders that should be substituted by test data.\n",
        "6. Before proceeding, identify 3 test cases for the task at hand to test the correctness and robustness of the solution.\n",
        "7. Generate sufficient test information to run those test cases.\n",
        "8. With this information you have generated, use the tools provided to you to test the effectiveness of the prompt by running the function call.\n",
        "9. Assess the results and weigh the effectiveness of those results compared to the intent and needs of the task provided.\n",
        "10. Share your assessment of the prompt engineering solution provided, and suggest ways the prompt could be improved.\n",
        "11. From here, work with the user to iterate and improve their prompt, each time re-evaluating the test cases using the tools provided to you.\n",
        "12. Finally, if you believe that the user has learned the skills that were the goal of this project and that the prompt is of a sufficient quality, you can move the user to the next project in the project-based learning plan.\n",
        "13. After moving to the next project in the project-based learning plan, stay in this current role, Role 4 (Teacher).\n",
        "'''\n",
        "\n",
        "teacher_prompt += tool_prompt\n",
        "\n",
        "teacher_prompt += '''\n",
        "Here is an example of a task you can give to a user:\n",
        "<example_task>\n",
        "<question> Write a prompt that translates {{LANGUAGE 1}} into {{LANGUAGE 2}}. </question>\n",
        "</example_task>\n",
        "\n",
        "Here is an example of a Claude prompt that you may receive as input from the user.\n",
        "This example input is only for illustrative purposes, and should not be considered an actual input from the user:\n",
        "<example_prompt>\n",
        "Your role is to be an experienced newsletter writer who summarizes long articles into short, concise bullet points.\n",
        "\n",
        "Keep your answer as short as possible.\n",
        "Distill the information down to at most 5 bullet points.\n",
        "Keep your information as factual as possible, and do not extrapolate or share your thoughts on the content itself.\n",
        "\n",
        "Here is the article to be summarized: {{ARTICLE}}\n",
        "</example_prompt>\n",
        "\n",
        "Here is how you should format your output when in your Skill-gauging Role and Teacher roles:\n",
        "- Generate test cases and run the prompt tests by using function calls to the tools provided above. Your output should only be the function call.\n",
        "- Assess the results of your function calls in and show as much work as possible in <scratchpad> tags before sharing a final, comprehensive answer in <answer> tags.\n",
        "\n",
        "Here is how you should format your output when in your Curriculum Planner and Material Generator roles:\n",
        "- Show all of your work and logic and keep a running log of all of your inputs and thinking in <scratchpad> tags before sharing your answer in <answer> tags.\n",
        "\n",
        "In general, think step-by-step. Please be as verbose as possible and explain any of your thinking.\n",
        "\n",
        "The first input from the user will be their name. Your first response should be a greeting and a restatement of your roles and goals in each role, written as an introductory message for the user. Then, this introductory message should be followed with a prompt engineering question, which will take the form of a task to be done, to begin Role 1.\n",
        "The expected input from the user to answer this should be a prompt that can be run using the function calls given to you to handle this task. Any questions should be addressed.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic\n",
        "!pip install colorama"
      ],
      "metadata": {
        "id": "Dmtu-cIm9NZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154abb85-fd3c-40cd-8d48-28b32bc0d647"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.19.2-py3-none-any.whl (850 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.2/850.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from anthropic)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.6.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->anthropic)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.16.3)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)\n",
            "Installing collected packages: h11, httpcore, httpx, anthropic\n",
            "Successfully installed anthropic-0.19.2 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "import json\n",
        "import re\n",
        "from google.colab import userdata\n",
        "from colorama import Fore, Back, Style\n",
        "\n",
        "#def teacher_claude():\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "    api_key = userdata.get(\"CLAUDE_API_KEY\")\n",
        ")\n",
        "\n",
        "# Conversation History\n",
        "conversation_history = []\n",
        "\n",
        "# Configuration Variables (Note this should really only impact the conversation)\n",
        "MAX_TOKENS = 4096\n",
        "TEMPERATURE = 0\n",
        "SYSTEM_PROMPT = teacher_prompt\n",
        "MODEL = \"claude-3-sonnet-20240229\"\n",
        "\n",
        "def conversation(history):\n",
        "  return client.messages.create(model=MODEL,\n",
        "                              system=SYSTEM_PROMPT,\n",
        "                              max_tokens=MAX_TOKENS,\n",
        "                              temperature=TEMPERATURE,\n",
        "                              stop_sequences=[\"</run_prompt>\"],\n",
        "                              messages=history)\n",
        "\n",
        "# Again, from the Anthropic function calling book\n",
        "def extract_between_tags(tag: str, string: str, strip: bool = False) -> list[str]:\n",
        "    ext_list = re.findall(f\"<{tag}>(.+?)</{tag}>\", string, re.DOTALL)\n",
        "    if strip:\n",
        "        ext_list = [e.strip() for e in ext_list]\n",
        "    return ext_list\n",
        "\n",
        "# Begin conversation with user's name\n",
        "print(Fore.GREEN)\n",
        "message = 'Chiyoung'#input(\"Please input your name: \")\n",
        "print(Style.RESET_ALL)\n",
        "\n",
        "while True:\n",
        "\n",
        "  # Feed conversation history with User input into AI\n",
        "  conversation_history.append({\"role\":\"user\",\"content\": message})\n",
        "  conversation_response = conversation(conversation_history)\n",
        "  conversation_response_text = conversation_response.content[0].text # Extract text\n",
        "\n",
        "  # Check if running a prompt\n",
        "  if conversation_response.stop_sequence == \"</run_prompt>\":\n",
        "\n",
        "    prompt_to_run = conversation_response_text.split(\"<run_prompt>\")\n",
        "    if len(prompt_to_run) > 1: prompt_to_run = prompt_to_run[1]\n",
        "    else: prompt_to_run = prompt_to_run[0]\n",
        "\n",
        "    # Get ready to extract data\n",
        "    prompt_to_run = prompt_to_run.replace('\\n',' ')\n",
        "    prompt = extract_between_tags(tag=\"prompt\",string=prompt_to_run)[0]\n",
        "    test_vals = extract_between_tags(tag=\"test\",string=prompt_to_run)[0]\n",
        "    test_vals = json.loads(test_vals)\n",
        "    test_vals = test_vals[\"test\"]\n",
        "\n",
        "    # Use the test cases to generate the test prompts\n",
        "    prompts_to_eval=[]\n",
        "    for test in test_vals:\n",
        "      add_prompt = prompt\n",
        "      for key, value in list(test.items()):\n",
        "        add_prompt = add_prompt.replace(key,json.dumps(value))\n",
        "      prompts_to_eval.append(add_prompt)\n",
        "\n",
        "    # Evaluate the prompts and append the evaluation prompt and results\n",
        "    evals = []\n",
        "    for eval_prompt in prompts_to_eval:\n",
        "      evaluation = client.messages.create(model=MODEL,\n",
        "                                  max_tokens=MAX_TOKENS,\n",
        "                                  temperature=TEMPERATURE,\n",
        "                                  messages=[{\"role\":\"user\", \"content\":eval_prompt}])\n",
        "      evaluation = evaluation.content[0].text\n",
        "      evals.append((eval_prompt, evaluation))\n",
        "\n",
        "    # Synthesize the AI response\n",
        "    text_output = conversation_response.content[0].text\n",
        "    text_output += '\\n</run_prompt>'\n",
        "    counter = 1\n",
        "    for eval_prompt, result in evals:\n",
        "      text_output += f'\\n<test_{counter}>\\n'\n",
        "      text_output += eval_prompt\n",
        "      text_output += f'\\n</test_{counter}>'\n",
        "      text_output += f'\\n<result_{counter}>\\n'\n",
        "      text_output += result\n",
        "      text_output += f'\\n</result_{counter}>\\n'\n",
        "      counter += 1\n",
        "    text_output += f'<scratchpad>\\nI will now assess the results of the test cases.'\n",
        "\n",
        "    complete_prompt_run = conversation_history.copy()\n",
        "    complete_prompt_run.append({\"role\":\"assistant\", \"content\":text_output})\n",
        "\n",
        "    # Finish the half-complete response, now with the prompt results\n",
        "    test_response = client.messages.create(model=MODEL,\n",
        "                                  max_tokens=MAX_TOKENS,\n",
        "                                  temperature=TEMPERATURE,\n",
        "                                  messages=complete_prompt_run)\n",
        "\n",
        "    # Pass the fully completed message back\n",
        "    conversation_response_text = text_output + '/n' + test_response.content[0].text\n",
        "\n",
        "  # Update conversation history with AI response\n",
        "  conversation_history.append({\"role\":\"assistant\",\"content\": conversation_response_text})\n",
        "\n",
        "  # CLAUDE OUTPUT\n",
        "  print(\"Claude: \" + conversation_response_text)\n",
        "  #print(Fore.RED + \"Claude: \" + conversation_display + Style.RESET_ALL)\n",
        "\n",
        "  # USER INPUT\n",
        "  print(Fore.GREEN)\n",
        "  message = input(\"User: \")\n",
        "  print(Style.RESET_ALL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "GvbbIts49zBL",
        "outputId": "a8712364-fbce-4f25-d523-1c48d41410c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'teacher_prompt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c2ee8323719a>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mMAX_TOKENS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mTEMPERATURE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mSYSTEM_PROMPT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mMODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"claude-3-sonnet-20240229\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'teacher_prompt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nlr83eyALOg0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}