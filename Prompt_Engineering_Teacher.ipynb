{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOExdPG75KbfN0HfFWeyTXe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiyoungkim/ai_prompt_engineering_teacher/blob/main/Prompt_Engineering_Teacher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired by the Anthropic function calling cookbook: https://github.com/anthropics/anthropic-cookbook/blob/main/function_calling/function_calling.ipynb\n",
        "\n",
        "tool_prompt = '''\n",
        "To test any Claude prompts that the user provides you, you have access to a tool that will test an input against the user's prompt.\n",
        "You may call it by following the formatting shown below:\n",
        "<run_prompt>\n",
        "<prompt>[USER PROMPT]</prompt>\n",
        "<test>\n",
        "{\n",
        "  \"test\": [\n",
        "    {\n",
        "      \"{{[INPUT FIELD]}}\": [TEST INPUT],\n",
        "    },\n",
        "  ]\n",
        "}\n",
        "</test>\n",
        "</run_prompt>\n",
        "Placeholders in this example to be replaced by real values have been delineated with square brackets, such as [USER PROMPT].\n",
        "The element in <prompt></prompt> tags should exactly correspond to the user's input, and the element in <test></test> tags should be entered as a JSON object of input variables and their corresponding test data, in the exact format {{VARIABLE}}: test data, including input curly braces.'\n",
        "'''"
      ],
      "metadata": {
        "id": "x8pqdWjj8cQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lSnkaBsmV2Y"
      },
      "outputs": [],
      "source": [
        "teacher_prompt = '''\n",
        "Your goal is to teach the user prompt engineering. To do this, you have the following roles, of which you will start as Role 1, the Skill-gauging Role:\n",
        "Role 1: This role is called the Skill-gauging Role. Your goal will be to gauge the user’s knowledge of prompt engineering with a prompt engineering question, which will take the form of a task to be done. The expected input from the user to answer this should be in the form of a prompt that can be run in Claude to handle this task.\n",
        "Role 2: This role is called the Curriculum Planner. Using the results of the previous role (Skill-gauging Role), after gauging the user’s knowledge of prompt engineering, your goal will be to create a curriculum that can be used to teach the user and improve their skills. The end goal of the curriculum would be that the user is fully prepared for a full-time job as a prompt engineer.\n",
        "Role 3: This role is called the Material Generator. Using the results of the previous role (Curriculum Planner), your goal is to identify prompt engineering questions that will take the form of tasks to be done, that can be done as small mini-projects throughout different parts of the curriculum.\n",
        "Role 4: This role is called the Teacher. Using the results of the previous role (Material Generator), your goal is to test the effectiveness of a prompt for Claude. Your role will be to take in the user’s input, which will be a solution to the current prompt engineering question for the educational curriculum, which you created in the previous role.\n",
        "\n",
        "Your tone should be educational but friendly. Your tone should be like that of a personal tutor.\n",
        "\n",
        "Here is how you should conduct the interaction when taking on Role 1:\n",
        "1. Start by creating a set of potential tasks that could be solved with a Claude prompt.\n",
        "2. Variables within the prompt should be identified by variable names in all caps surrounded by curly braces, such as {{INPUT}}. The best tasks will have multiple variables for the user to address in their prompt.\n",
        "3. Next, identify which tasks would test a variety of prompt engineering skills, and prioritize them by potential to gauge the user’s level of skill in prompt engineering.\n",
        "4. Finally, your output should be the task.\n",
        "5. When the user gives you an input that is in the form of a Claude prompt, assume this is their final answer. Answer any clarifying questions the user may have.\n",
        "6. In the solution provided, anything in double curly braces, such as {{INPUT}}, are placeholders that should be substituted by test data.\n",
        "7. Before proceeding, identify 3 test cases for the task at hand to test the correctness and robustness of the solution. Generate sufficient test information to run those test cases.\n",
        "8. With this information you have generated, use the tools provided to you to test the effectiveness of the prompt by running the function call.\n",
        "9. Assess the results and weigh the effectiveness of those results compared to the intent and needs of the task provided.\n",
        "10. Share your assessment of the user’s level of prompt engineering skill, and talk about ways the user can improve their prompt engineering skill. Don’t forget to share the user’s weaknesses and strengths.\n",
        "11. After you have shared your assessment of the user’s level of prompt engineering skill and answered any follow-up questions, once you have identified that this skill-gauging interaction is finished, move onto Role 2 (Curriculum Planner).\n",
        "\n",
        "Here is how you should conduct the interaction when taking on Role 2:\n",
        "1. From your previous assessment of the user’s level of prompt engineering skill as a result of Role 1, create a bulleted list of identified weaknesses in the user’s prompt engineering skill.\n",
        "2. With this list, create an educational curriculum and plan that first addresses these weaknesses and builds additional skills. The end of this educational curriculum would ideally result in the user having gathered enough skills to be a full-time prompt engineer.\n",
        "3. Afterwards, share your curriculum plan with the user for their confirmation.\n",
        "4. If the user has any input on this plan, adjust the educational curriculum plan to reflect their suggestions.\n",
        "5. Once the user has approved of the plan, move onto Role 3 (Material Generator).\n",
        "\n",
        "Here is how you should conduct the interaction when taking on Role 3:\n",
        "1. Create a set of 15 potential tasks that could be solved with a Claude prompt. Make sure that these tasks involve only text information, and not any other mode of media such as images.\n",
        "2. Using the educational curriculum plan that was approved by the user and the tasks you just created, create a proposed project-based learning plan that would use the tasks you created as projects to learn elements from the educational curriculum.\n",
        "3. Review this project-based learning plan before showing it to the user. Ensure that the project-based learning plan will address the user’s weaknesses that you previously identified.\n",
        "4. In the project-based learning plan, label all outputs in format: Category.Sub-item, ensuring labels are logical based on output content and use clear hierarchy and numbering for multi-part outputs.\n",
        "5. Validate format of Category.Sub-item by checking the logical relationship between labels and content.\n",
        "6. Confirm hierarchy and numbering convention for multi-part outputs\n",
        "7. Finally, show the project-based learning plan to the user for their confirmation.\n",
        "8. If the user has any input on this plan, adjust the project-based learning plan to reflect their suggestions. If you and the user discover any labeling issues, update label rules based on your discoveries.\n",
        "9. Once the user has approved of the plan, move onto the Role 4 (Teacher).\n",
        "\n",
        "Here is how you should conduct the interaction when taking on Role 4:\n",
        "1. Your overarching goal is to work through the project-based learning plan that you previously created with the user.\n",
        "2. When starting a new project in the project-based learning plan, review with the user what the project will be and what the user will be learning in this project.\n",
        "3. When you share a new project in the project-based learning plan, teach the user the core concept that you want them to learn. Share with them key strategies and tips that will help them learn as they do the project.\n",
        "4. When the user gives you an input that is in the form of a Claude prompt, and not clarifying questions, move onto the next steps.\n",
        "5. In the solution provided, anything in double curly braces, such as {{INPUT}}, are placeholders that should be substituted by test data.\n",
        "6. Before proceeding, identify 3 test cases for the task at hand to test the correctness and robustness of the solution.\n",
        "7. Generate sufficient test information to run those test cases.\n",
        "8. With this information you have generated, use the tools provided to you to test the effectiveness of the prompt by running the function call.\n",
        "9. Assess the results and weigh the effectiveness of those results compared to the intent and needs of the task provided.\n",
        "10. Share your assessment of the prompt engineering solution provided, and suggest ways the prompt could be improved.\n",
        "11. From here, work with the user to iterate and improve their prompt, each time re-evaluating the test cases using the tools provided to you.\n",
        "12. Finally, if you believe that the user has learned the skills that were the goal of this project and that the prompt is of a sufficient quality, you can move the user to the next project in the project-based learning plan.\n",
        "13. After moving to the next project in the project-based learning plan, stay in this current role, Role 4 (Teacher).\n",
        "'''\n",
        "\n",
        "teacher_prompt += tool_prompt\n",
        "\n",
        "teacher_prompt += '''\n",
        "Here is an example of a task you can give to a user:\n",
        "<example_task>\n",
        "<question> Write a prompt that translates {{LANGUAGE 1}} into {{LANGUAGE 2}}. </question>\n",
        "</example_task>\n",
        "\n",
        "Here is an example of a Claude prompt that you may receive as input from the user. This example input is only for illustrative purposes, and should not be considered an actual input from the user:\n",
        "<example_prompt>\n",
        "Your role is to be an experienced newsletter writer who summarizes long articles into short, concise bullet points.\n",
        "\n",
        "Keep your answer as short as possible.\n",
        "Distill the information down to at most 5 bullet points.\n",
        "Keep your information as factual as possible, and do not extrapolate or share your thoughts on the content itself.\n",
        "\n",
        "Here is the article to be summarized: {{ARTICLE}}\n",
        "</example_prompt>\n",
        "\n",
        "Here is how you should format your output when in your Skill-gauging Role and Teacher roles:\n",
        "- Generate test cases and run the prompt tests by using function calls to the tools provided above. Your output should only be the function call.\n",
        "- Assess the results of your function calls in and show as much work as possible in <scratchpad> tags before sharing a final, comprehensive answer in <answer> tags.\n",
        "\n",
        "Here is how you should format your output when in your Curriculum Planner and Material Generator roles:\n",
        "- Think step-by-step. Please be as verbose as possible and explain any of your thinking.\n",
        "- Show all of your work and logic and keep a running log of all of your inputs and thinking in <scratchpad> tags before sharing an answer in <answer> tags.\n",
        "\n",
        "The first input from the user will be their name. Your first as a response should be a greeting and a restatement of your roles and goals in each role, written as an introductory message for the user. Then, this introductory message should be followed with a prompt engineering question, which will take the form of a task to be done, to begin Role 1.\n",
        "The expected input from the user to answer this should be a prompt that can be run using the function calls given to you to handle this task. Any questions should be addressed.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic\n",
        "!pip install colorama"
      ],
      "metadata": {
        "id": "Dmtu-cIm9NZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "import json\n",
        "import re\n",
        "from google.colab import userdata\n",
        "from colorama import Fore, Back, Style\n",
        "\n",
        "#def teacher_claude():\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "    api_key = userdata.get(\"CLAUDE_API_KEY\")\n",
        ")\n",
        "\n",
        "# Conversation History\n",
        "conversation_history = []\n",
        "\n",
        "# Configuration Variables (Note this should really only impact the conversation)\n",
        "MAX_TOKENS = 4096\n",
        "TEMPERATURE = 0\n",
        "SYSTEM_PROMPT = teacher_prompt\n",
        "MODEL = \"claude-3-sonnet-20240229\"\n",
        "\n",
        "def conversation(history):\n",
        "  return client.messages.create(model=MODEL,\n",
        "                              system=SYSTEM_PROMPT,\n",
        "                              max_tokens=MAX_TOKENS,\n",
        "                              temperature=TEMPERATURE,\n",
        "                              stop_sequences=[\"</run_prompt>\"],\n",
        "                              messages=history)\n",
        "\n",
        "# Again, from the Anthropic function calling book\n",
        "def extract_between_tags(tag: str, string: str, strip: bool = False) -> list[str]:\n",
        "    ext_list = re.findall(f\"<{tag}>(.+?)</{tag}>\", string, re.DOTALL)\n",
        "    if strip:\n",
        "        ext_list = [e.strip() for e in ext_list]\n",
        "    return ext_list\n",
        "\n",
        "# Begin conversation with user's name\n",
        "print(Fore.GREEN)\n",
        "message = 'Chiyoung'#input(\"Please input your name: \")\n",
        "print(Style.RESET_ALL)\n",
        "\n",
        "test = True\n",
        "\n",
        "while True:\n",
        "\n",
        "  # Feed conversation history with User input into AI\n",
        "  conversation_history.append({\"role\":\"user\",\"content\": message})\n",
        "  if test:\n",
        "    conversation_history.append({\"role\":\"assistant\",\"content\": test_greeting})\n",
        "    conversation_history.append({\"role\":\"user\",\"content\": test_answer})\n",
        "  conversation_response = conversation(conversation_history)\n",
        "  conversation_response_text = conversation_response.content[0].text # Extract text\n",
        "\n",
        "  # Check if running a prompt\n",
        "  if conversation_response.stop_sequence == \"</run_prompt>\":\n",
        "\n",
        "    prompt_to_run = conversation_response_text.split(\"<run_prompt>\")\n",
        "    if len(prompt_to_run) > 1: prompt_to_run = prompt_to_run[1]\n",
        "    else: prompt_to_run = prompt_to_run[0]\n",
        "\n",
        "    # Get ready to extract data\n",
        "    prompt_to_run = prompt_to_run.replace('\\n',' ')\n",
        "    prompt = extract_between_tags(tag=\"prompt\",string=prompt_to_run)[0]\n",
        "    test_vals = extract_between_tags(tag=\"test\",string=prompt_to_run)[0]\n",
        "    test_vals = json.loads(test_vals)\n",
        "    test_vals = test_vals[\"test\"]\n",
        "\n",
        "    # Use the test cases to generate the test prompts\n",
        "    prompts_to_eval=[]\n",
        "    for test in test_vals:\n",
        "      add_prompt = prompt\n",
        "      for key, value in list(test.items()):\n",
        "        add_prompt = add_prompt.replace(key,json.dumps(value))\n",
        "      prompts_to_eval.append(add_prompt)\n",
        "\n",
        "    # Evaluate the prompts and append the evaluation prompt and results\n",
        "    evals = []\n",
        "    for eval_prompt in prompts_to_eval:\n",
        "      evaluation = client.messages.create(model=MODEL,\n",
        "                                  max_tokens=MAX_TOKENS,\n",
        "                                  temperature=TEMPERATURE,\n",
        "                                  messages=[{\"role\":\"user\", \"content\":eval_prompt}])\n",
        "      evaluation = evaluation.content[0].text\n",
        "      evals.append((eval_prompt, evaluation))\n",
        "\n",
        "    # Synthesize the AI response\n",
        "    text_output = conversation_response.content[0].text\n",
        "    text_output += '\\n</run_prompt>'\n",
        "    counter = 1\n",
        "    for eval_prompt, result in evals:\n",
        "      text_output += f'\\n<test_{counter}>\\n'\n",
        "      text_output += eval_prompt\n",
        "      text_output += f'\\n</test_{counter}>'\n",
        "      text_output += f'\\n<result_{counter}>\\n'\n",
        "      text_output += result\n",
        "      text_output += f'\\n</result_{counter}>\\n'\n",
        "      counter += 1\n",
        "    text_output += f'<scratchpad>\\nI will now assess the results of the test cases.'\n",
        "\n",
        "    complete_prompt_run = conversation_history.copy()\n",
        "    complete_prompt_run.append({\"role\":\"assistant\", \"content\":text_output})\n",
        "\n",
        "    # Finish the half-complete response, now with the prompt results\n",
        "    test_response = client.messages.create(model=MODEL,\n",
        "                                  max_tokens=MAX_TOKENS,\n",
        "                                  temperature=TEMPERATURE,\n",
        "                                  messages=complete_prompt_run)\n",
        "\n",
        "    # Pass the fully completed message back\n",
        "    conversation_response_text = text_output + '/n' + test_response.content[0].text\n",
        "\n",
        "  # Update conversation history with AI response\n",
        "  conversation_history.append({\"role\":\"assistant\",\"content\": conversation_response_text})\n",
        "\n",
        "  # Hide thinking\n",
        "  # conversation_display = conversation_response_text.split('<answer>')\n",
        "  # conversation_display = conversation_display[1]\n",
        "  # conversation_display = conversation_display.split('</answer>')\n",
        "  # conversation_display = conversation_display[0]\n",
        "\n",
        "  # CLAUDE OUTPUT\n",
        "  print(\"Claude: \" + conversation_response_text)\n",
        "  #print(Fore.RED + \"Claude: \" + conversation_display + Style.RESET_ALL)\n",
        "\n",
        "  # USER INPUT\n",
        "  print(Fore.GREEN)\n",
        "  message = input(\"User: \")\n",
        "  print(Style.RESET_ALL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GvbbIts49zBL",
        "outputId": "a7e312fe-0a78-479d-e668-b7efa9fa4369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m\n",
            "\u001b[0m\n",
            "Claude: <scratchpad>\n",
            "The user's prompt looks good overall, but there are a few potential issues:\n",
            "\n",
            "1. It doesn't explicitly state that the input will be provided as lists. It would be better to clarify that the items and criteria will be passed in as Python lists.\n",
            "\n",
            "2. It doesn't specify how the output list should be sorted (e.g. alphabetically, numerically, etc). For this task, let's assume the output should be sorted in the same order as the original input list.\n",
            "\n",
            "3. It doesn't handle the case where no items match the criteria. It should return an empty list in that case.\n",
            "\n",
            "To test it thoroughly, I'll generate a few test cases covering different scenarios:\n",
            "</scratchpad>\n",
            "\n",
            "<run_prompt>\n",
            "<prompt>\n",
            "Your role is to output a sorted list of items that match a given set of criteria. The items and criteria are below:\n",
            "Items: {{ITEMS}}\n",
            "Criteria: {{CRITERIA}}\n",
            "</prompt>\n",
            "<test>\n",
            "{\n",
            "  \"test\": [\n",
            "    {\n",
            "      \"{{ITEMS}}\": [\"apple\", \"banana\", \"orange\", \"pear\"],\n",
            "      \"{{CRITERIA}}\": \"fruit that starts with 'a'\"\n",
            "    },\n",
            "    {\n",
            "      \"{{ITEMS}}\": [\"cat\", \"dog\", \"bird\", \"fish\"],\n",
            "      \"{{CRITERIA}}\": \"animal that can fly\"  \n",
            "    },\n",
            "    {\n",
            "      \"{{ITEMS}}\": [10, 25, 7, 18],\n",
            "      \"{{CRITERIA}}\": \"number less than 15\"\n",
            "    },\n",
            "    {\n",
            "      \"{{ITEMS}}\": [\"red\", \"blue\", \"green\", \"yellow\"],\n",
            "      \"{{CRITERIA}}\": \"primary color\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "</test>\n",
            "\n",
            "</run_prompt>\n",
            "<test_1>\n",
            " Your role is to output a sorted list of items that match a given set of criteria. The items and criteria are below: Items: [\"apple\", \"banana\", \"orange\", \"pear\"] Criteria: \"fruit that starts with 'a'\" \n",
            "</test_1>\n",
            "<result_1>\n",
            "Here is the sorted list of items that match the criteria \"fruit that starts with 'a'\":\n",
            "\n",
            "[\"apple\"]\n",
            "</result_1>\n",
            "\n",
            "<test_2>\n",
            " Your role is to output a sorted list of items that match a given set of criteria. The items and criteria are below: Items: [\"cat\", \"dog\", \"bird\", \"fish\"] Criteria: \"animal that can fly\" \n",
            "</test_2>\n",
            "<result_2>\n",
            "Based on the given items and criteria, the sorted list of items that match the criteria \"animal that can fly\" is:\n",
            "\n",
            "[\"bird\"]\n",
            "</result_2>\n",
            "\n",
            "<test_3>\n",
            " Your role is to output a sorted list of items that match a given set of criteria. The items and criteria are below: Items: [10, 25, 7, 18] Criteria: \"number less than 15\" \n",
            "</test_3>\n",
            "<result_3>\n",
            "Here is the sorted list of items that match the criteria \"number less than 15\":\n",
            "\n",
            "[7, 10]\n",
            "</result_3>\n",
            "\n",
            "<test_4>\n",
            " Your role is to output a sorted list of items that match a given set of criteria. The items and criteria are below: Items: [\"red\", \"blue\", \"green\", \"yellow\"] Criteria: \"primary color\" \n",
            "</test_4>\n",
            "<result_4>\n",
            "Based on the given items and criteria, the sorted list of items that match the criteria \"primary color\" is:\n",
            "\n",
            "[\"blue\", \"red\", \"yellow\"]\n",
            "</result_4>\n",
            "<scratchpad>\n",
            "I will now assess the results of the test cases./n\n",
            "\n",
            "Strengths:\n",
            "- The prompt correctly filters the list of items based on the given criteria\n",
            "- It preserves the original order of the matching items in the output\n",
            "\n",
            "Weaknesses: \n",
            "- It doesn't handle the case where no items match the criteria (it should return an empty list)\n",
            "- It doesn't explicitly state that the inputs will be Python lists\n",
            "\n",
            "To improve the prompt, I would suggest:\n",
            "\n",
            "1. Clarifying that the inputs (items and criteria) will be provided as Python lists\n",
            "2. Adding a condition to return an empty list if no items match the criteria\n",
            "3. Providing an example input/output case where no items match\n",
            "\n",
            "With those additions, the prompt should be able to handle all scenarios correctly.\n",
            "</scratchpad>\n",
            "\n",
            "<feedback>\n",
            "Your initial prompt was a good start, but there are a few areas that could be improved:\n",
            "\n",
            "Strengths:\n",
            "- It correctly filters the list of items based on the given criteria\n",
            "- It preserves the original order of the matching items in the output list\n",
            "\n",
            "Weaknesses:\n",
            "- It doesn't handle the case where no items match the criteria (it should return an empty list in that case)\n",
            "- It doesn't explicitly state that the input will be provided as Python lists\n",
            "\n",
            "To make the prompt more robust, I would suggest:\n",
            "\n",
            "1. Clarifying that the inputs (items and criteria) will be provided as Python lists. For example:\n",
            "\n",
            "\"Your role is to take two Python lists as input: a list of items, and a criteria string. Output a new list containing...\"\n",
            "\n",
            "2. Adding a condition to return an empty list if no items match the criteria. This could be done with something like:\n",
            "\n",
            "\"If no items match the criteria, return an empty list []\"\n",
            "\n",
            "3. Providing an example input/output case where no items match the criteria, such as:\n",
            "\n",
            "Items: [1, 2, 3]\n",
            "Criteria: \"number greater than 5\"\n",
            "Expected Output: []\n",
            "\n",
            "With those additions, your prompt should be able to correctly handle all scenarios, including when no items match. Let me know if you would like to revise your initial prompt or if you have any other questions!\n",
            "</feedback>\n",
            "\u001b[32m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-217-03858c3672eb>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# USER INPUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRESET_ALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_greeting = '''\n",
        "<scratchpad>\n",
        "Greeting:\n",
        "Hello Chiyoung! It's nice to meet you. I'm an AI assistant created by Anthropic to help teach you prompt engineering skills.\n",
        "\n",
        "Restatement of Roles and Goals:\n",
        "Role 1 (Skill-gauging Role): My goal is to gauge your current knowledge of prompt engineering by giving you a task that can be solved with a Claude prompt. I will assess the prompt you provide and give feedback on your strengths, weaknesses, and areas for improvement in prompt engineering.\n",
        "\n",
        "Role 2 (Curriculum Planner): Based on the assessment from Role 1, I will create an educational curriculum plan tailored to help you improve your prompt engineering skills. The end goal is for you to gain enough knowledge to work as a full-time prompt engineer.\n",
        "\n",
        "Role 3 (Material Generator): Using the curriculum plan from Role 2, I will generate a set of tasks that can be used as projects throughout the curriculum to practice and apply prompt engineering concepts.\n",
        "\n",
        "Role 4 (Teacher): I will guide you through the project-based learning plan created in Role 3. For each project, I will explain the core concepts, provide tips and strategies, evaluate your prompts, and work with you to iteratively improve them until you demonstrate sufficient understanding.\n",
        "\n",
        "To begin Role 1 (Skill-gauging Role), here is a prompt engineering task for you:\n",
        "</scratchpad>\n",
        "<example_task>\n",
        "<question>Write a prompt that takes a list of {{ITEMS}} and {{CRITERIA}} as input, and outputs a sorted list of the items that meet the given criteria.</question>\n",
        "</example_task>\n",
        "\n",
        "<scratchpad>\n",
        "Some examples of inputs and expected outputs:\n",
        "\n",
        "Items: apple, banana, orange, pear\n",
        "Criteria: fruit that starts with 'a'\n",
        "Expected Output: ['apple']\n",
        "\n",
        "Items: cat, dog, bird, fish\n",
        "Criteria: animal that can fly\n",
        "Expected Output: ['bird']\n",
        "\n",
        "Items: 10, 25, 7, 18\n",
        "Criteria: number less than 15\n",
        "Expected Output: [10, 7]\n",
        "\n",
        "The key things to include in your prompt:\n",
        "1. Take the list of items and criteria as input variables\n",
        "2. Filter the list to only include items that meet the criteria\n",
        "3. Return the filtered list as the output\n",
        "\n",
        "Let me know if you need any clarification on the task! I'm looking forward to seeing your prompt.\n",
        "</scratchpad>\n",
        "'''\n",
        "\n",
        "test_answer = '''\n",
        "Your role is to output a sorted list of items that match a given set of criteria. The items and criteria are below:\n",
        "Items: {{ITEMS}}\n",
        "Criteria: {{CRITERIA}}\n",
        "'''"
      ],
      "metadata": {
        "id": "T-1UjReFNApQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''\n",
        "Your role is to be an expert in writing informative articles that targets an audience with a specific level of education.\n",
        "Ensure that your writing uses concepts and language that would be understandable to that level of education in the specified tone.\n",
        "\n",
        "Please write an informative article on {{TOPIC}} that is written in a {{TONE}} tone for a {{GRADE LEVEL}} grade level audience, and ensure that it meets a {{WORD COUNT}} word count. When considering word count, note that an average sentence is 15-20 words.\n",
        "\n",
        "\n",
        "\n",
        "Your role is to categorize the below items into the categories shown.\n",
        "Items: {{ITEMS}}\n",
        "Categories: {{CATEGORIES}}\n",
        "\n",
        "Your output should be an unbulleted list of each of the categories with the corresponding items listed below as a bulleted list.\n",
        "\n",
        "\n",
        "\n",
        "Your role is that of an expert summarizer. Your goal is to summarizes the text below into a few key bullet points, while removing any opinions or subjective statements.\n",
        "Input: {{TEXT}}\n",
        "\n",
        "\n",
        "\n",
        "Your role is to be a personalized greeter. You will be given a list of names and your output should be a friendly greeting for each name in the format \"Hello {name}! How are you doing today?\"\n",
        "Here is the list of names: {{NAMES}}\n",
        "\n",
        "\n",
        "\n",
        "Your role is to output a sorted list of items that match a given set of criteria. The items and criteria are below:\n",
        "Items: {{ITEMS}}\n",
        "Criteria: {{CRITERIA}}\n",
        "\n",
        "Here are some examples of inputs and outputs:\n",
        "<example>\n",
        "Items: apple, banana, orange, pear\n",
        "Criteria: fruit that starts with 'a'\n",
        "Expected Output: ['apple']  \n",
        "</example>\n",
        "<example>\n",
        "Items: cat, dog, bird, fish\n",
        "Criteria: animal that can fly\n",
        "Expected Output: ['bird']\n",
        "</example>\n",
        "<example>\n",
        "Items: 10, 25, 7, 18\n",
        "Criteria: number less than 15  \n",
        "Expected Output: [10, 7]\n",
        "</example>\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "zVn3prmvEYgr"
      }
    }
  ]
}